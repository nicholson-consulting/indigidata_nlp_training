{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text classification\n",
    "\n",
    "Detecting patterns and using them to classify text is a key part of NLP. It is also \n",
    "an example of one of the central problems of machine learning: \"given some data, can I\n",
    "identify some features that help me to classify that data as having some property that I care about?\"\n",
    "\n",
    "In the context of NLP this might take the form of deciding whether an e-mail message is spam or not;\n",
    "deciding whether a customer review is positive or negative; deciding what a person's gender is\n",
    "based on their name; or which language a word is from.\n",
    "\n",
    "The goal of classification is to choose the correct _label_ for a given _input_. \n",
    "In basic classification tasks, each input is considered as being independent from all\n",
    "other inputs, and the classification labels are defined in advance. More complex \n",
    "classification tasks include extending the classification to multi-class \n",
    "classification (i.e. more than one label is allowed per input); open-class \n",
    "classification (classes need not be pre-defined); and sequence classification \n",
    "(a list of inputs are jointly classified).\n",
    "\n",
    "\n",
    "## Supervised classification\n",
    "If a classifier is built on training data with the correct label for each input,\n",
    "it is known as _supervised_ classification.\n",
    "\n",
    "The general framework for supervised classification involves a training stage and a prediction\n",
    "stage. During training, a feature extractor is used to convert each input value to a feature set. \n",
    "These feature sets capture the basic information about each input that should be used to classify it. \n",
    "Pairs of feature sets and labels are fed into the machine learning algorithm to generate a \n",
    "classification model.\n",
    "\n",
    "During the prediction stage, the same feature extractor is used to convert \n",
    "previously unseen inputs to feature sets. These feature sets are then \n",
    "fed into the model, which generates predicted labels.\n",
    "\n",
    "\n",
    "\n",
    "![Supervised classification workflow](supervised-classification.png)\n",
    "\n",
    "Figure from nltk.org/book\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a gender classifier for names\n",
    "\n",
    "In some languages male and female names have some distinctive features: \n",
    "For example, female names are more likely to end in _a_, _e_, and _i_, relative to male names\n",
    "which are more likely to end in _k_, _o_, _r_, _s_, or _t_.\n",
    "\n",
    "We're going to build a name gender classifier. The first thing we need for this is a 'feature extractor'. \n",
    "This is a function that we can give an input (e.g. a name) at it gives back a list of one or more features of the input (e.g. the last letter of the name).\n",
    "\n",
    "To work with the NLTK classifier that we're going to use, our feature extractor needs to produce output that looks like a dictionary of the form `{'feature name': 'feature value'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "  # This function returns the last character of the input it is given\n",
    "  return {'last_letter': word[-1]}\n",
    "\n",
    "gender_features('Dion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare a list of labeled data to use for training and testing our \n",
    "name classifier.\n",
    "\n",
    "We're going to start with a list of names from the NLTK corpus. They're probably mostly\n",
    "European, or at least from the global North."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "# For all the names in the dataset male.txt, label them as male and store then as a pair of the form `('name', 'gender')`\n",
    "# Do the same for the names in the dataset female.txt\n",
    "# Then join the two datasets together.\n",
    "labeled_nltk_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "print(len(labeled_nltk_names))\n",
    "\n",
    "\n",
    "# Finally, shuffle the list of labeled names\n",
    "import random\n",
    "random.shuffle(labeled_nltk_names)\n",
    "print(labeled_nltk_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some labeled inputs and we're ready to apply our feature extractor `gender_features`.\n",
    "\n",
    "It's a good idea to split the names data into a training set and a test set.\n",
    "\n",
    "We're also going to apply the feature extractor to get some data that looks like `({'feature name': 'feature value'}, 'label')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the feature extractor to our name data\n",
    "featuresets_nltk = [(gender_features(name),gender) for (name, gender) in labeled_nltk_names] # a tuple of features and labels\n",
    "print(len(featuresets_nltk),'names in the NLTK set')\n",
    "\n",
    "#split the data into test and training sets\n",
    "train_set_nltk_names, test_set_nltk_names = featuresets_nltk[:5000], featuresets_nltk[5000:]\n",
    "\n",
    "print(train_set_nltk_names[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use the feature set we just produced to train a 'feature classifier'. In this case we're going to use a _naive Bayes classifier_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier, classify\n",
    "nb_classifier_nltk = NaiveBayesClassifier.train(train_set_nltk_names)\n",
    "nb_classifier_nltk.show_most_informative_features(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_nltk.classify(gender_features('Dion'))\n",
    "# note that the classifier uses the features extracted from the name, not the name itself.\n",
    "# this means that it is really doing something like\n",
    "#nb_classifier.classify({'last_letter': 'n'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the classifier on some names that it might not have seen before. \n",
    "\n",
    "Let's try it on some names it might not have seen in the training data \n",
    "(we could go a check that these names aren't actually in the training data if we wanted to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(nb_classifier_nltk.classify(gender_features('Kahu')))\n",
    "print(nb_classifier_nltk.classify(gender_features('Kiri')))\n",
    "print(nb_classifier_nltk.classify(gender_features('Manakore')))\n",
    "print(nb_classifier_nltk.classify(gender_features('Aperahama')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ko wai tō ingoa?\n",
    "\n",
    "Next, we're going to load in a list of \"Māori names\" that we scraped from a website. We can have a look at the code for scraping this names in the notebook `getCorpora.ipynb`.\n",
    "\n",
    "Before we do anything with them, it's worth thinking a bit about where these came from and any issues that might be related to using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some \"Māori names\" that we scraped from a US website\n",
    "\n",
    "import ast # we'll use ast for turning strings that look like tuples into tuples\n",
    "\n",
    "with open('maoriNames.txt', 'r') as f:\n",
    "  labeled_maori_names = f.read().splitlines()\n",
    "  labeled_maori_names = [ast.literal_eval(item) for item in labeled_maori_names]\n",
    "\n",
    "random.shuffle(labeled_maori_names)\n",
    "print(labeled_maori_names[:4])\n",
    "print(len(labeled_maori_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll follow the same process as we did above.\n",
    "First we're going to use the feature extractor to convert the labeled names to a feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_maori = [(gender_features(name),gender) for (name, gender) in labeled_maori_names] # a tuple of features and labels\n",
    "print(len(featuresets_maori),'names')\n",
    "\n",
    "#split the data into test and training sets\n",
    "train_set_maori_names, test_set_maori_names = featuresets_maori[:70], featuresets_maori[70:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But rather than training our classifier on the new data, we're first going to look at how it performs if\n",
    "we apply it directly to the new data set.\n",
    "\n",
    "We can do this by using a tool from NLTK that uses our labeled test data to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy:',classify.accuracy(nb_classifier_nltk, test_set_maori_names),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the accuracy of the NLTK names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy:',classify.accuracy(nb_classifier_nltk, test_set_nltk_names),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what happens if we train the classifier on the set of Māori names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_maori = NaiveBayesClassifier.train(train_set_maori_names)\n",
    "nb_classifier_maori.show_most_informative_features(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy:',classify.accuracy(nb_classifier_maori, test_set_maori_names),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy:',classify.accuracy(nb_classifier_maori, test_set_nltk_names),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like training the classifier on the same sort of data as what we intend \n",
    "to use it has very little impact on its performance. \n",
    "(This is probably due to a combination of a very small data set and uninformative features for the problems we're trying to solve.) \n",
    "And it has made the performance on the NLTK data set worse.\n",
    "\n",
    "Clearly, the \"look at the last letter\" heuristic does't always work well.\n",
    "\n",
    "Try modifying the feature extractor part of the code below to see if you can come up with something that performs better for the Māori names data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "  return{'last_letter': name[-1],\n",
    "         'first_letter': name[0],\n",
    "         'length': len(name)\n",
    "           }\n",
    "\n",
    "featuresets_maori = [(gender_features(name),gender) for (name, gender) in labeled_maori_names] # a tuple of features and labels\n",
    "\n",
    "#split the data into test and training sets\n",
    "train_set_maori_names, test_set_maori_names = featuresets_maori[:70], featuresets_maori[70:]\n",
    "nb_classifier = NaiveBayesClassifier.train(test_set_maori_names)\n",
    "nb_classifier.show_most_informative_features(9)\n",
    "print('Accuracy:',classify.accuracy(nb_classifier, test_set_maori_names),'\\n')\n",
    "\n",
    "# if you want you can also test how well your classifier works on the NLTK names\n",
    "#print('Accuracy:',classify.accuracy(nb_classifier, test_set_nltk_names),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Choosing the right features\n",
    "\n",
    "Selecting relevant features can have an enormous impact on a learning method's ability to\n",
    "extract a good model. Most of the work of building a good classifier is in deciding what features might relevant\n",
    "and how they can best be represented in conjunction with other features.\n",
    "\n",
    "This process of designing a feature extractor involves a process of trial-and error. \n",
    "A common approach is to start with all the possible features that you can think of\n",
    "and the refine the feature extractor to only those which are actually helpful.\n",
    "\n",
    "However, there are limits to the number of features that you should train on.\n",
    "Providing too many features risks _over-fitting_ which can lead to an uninformative \n",
    "classifier when it is extended from the training set to the test set and \n",
    "can result in worse performance than a classifier that uses a smaller number of features.\n",
    "\n",
    "_You can try building and using a feature extractor that checks for the presence of every letter of the alphabet\n",
    "as a feature._\n",
    "\n",
    "Hint: you can use something like\n",
    "```\n",
    "def extract_features(name):\n",
    "  features = {}\n",
    "  for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    features['includes({})'.format(letter)] = letter in name.lower()\n",
    "  return features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(name):\n",
    "  features = {}\n",
    "  for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    features['includes({})'.format(letter)] = letter in name.lower()\n",
    "  return features\n",
    "\n",
    "extract_features('Dion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test the feature extractor above on the names data, or write your own one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying reo\n",
    "\n",
    "Now that we've had an introduction to the how classifiers work and how they can be applied to text, we can look at trying to build a reo classifier that can be given a word and will classify it as either Māori or English.\n",
    "\n",
    "To start with, we're going to need some labeled data. We've made this by taking two documents, converting them to words and then tagging those words. One document we have assumed is entirely kupu Māori, the other we have assumed is entirely English words.\n",
    "\n",
    "If you want, you can look more closely at how the text data was collected.\n",
    "Details are in `getCorpora.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and format labeled kupu Māori\n",
    "with open('kupuMaori.txt', 'r', encoding='utf8') as f:\n",
    "  labeled_maori_text = f.read().splitlines()\n",
    "  labeled_maori_text = [ast.literal_eval(item) for item in labeled_maori_text]\n",
    "\n",
    "random.shuffle(labeled_maori_text)\n",
    "print(labeled_maori_text[:5])\n",
    "print(len(labeled_maori_text))\n",
    "\n",
    "# load and format labeled English words\n",
    "with open('englishWords.txt', 'r', encoding='utf8') as f:\n",
    "  labeled_english_text = f.read().splitlines()\n",
    "  labeled_english_text = [ast.literal_eval(item) for item in labeled_english_text]\n",
    "\n",
    "random.shuffle(labeled_english_text)\n",
    "print(labeled_english_text[:5])\n",
    "print(len(labeled_english_text))\n",
    "\n",
    "# combine the Māori and English kupu\n",
    "labeled_text = labeled_maori_text + labeled_english_text\n",
    "random.shuffle(labeled_text)\n",
    "print(labeled_text[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a feature extractor to use with our text. \n",
    "One possibility is to start with the example above - `extract_features()` where the presence or absence of each letter is a feature.\n",
    "\n",
    "Or we could try something new of our own devising..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kupu_features(kupu):\n",
    "  features = {}\n",
    "  features['first_letter'] = kupu[0]\n",
    "  features['last_letter'] = kupu[-1]\n",
    "  features['has_macron'] = True in [macron_letter in kupu.lower() for macron_letter in 'āēīōū']\n",
    "  features['english_only_letters'] = True in [english_letter in kupu.lower() for english_letter in 'bcdfjlqsvxyz']\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check our possible feature extractors\n",
    "\n",
    "print(extract_features('consideration'))\n",
    "\n",
    "print(extract_kupu_features('consideration'))\n",
    "\n",
    "print(extract_features('rōpū'))\n",
    "\n",
    "print(extract_kupu_features('rōpū'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try applying one of our feature extractors to our corpus of mixed Māori and English words.\n",
    "We'll follow a similar process to above where we split the data into some for training and some for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featureset_kupu= [(extract_kupu_features(kupu.lower()),reo) for (kupu, reo) in labeled_text] # a tuple of features and labels\n",
    "print(len(featureset_kupu))\n",
    "\n",
    "#split the data into test and training sets\n",
    "train_set_kupu, test_set_kupu = featureset_kupu[:3000], featureset_kupu[3000:]\n",
    "print(labeled_text[:3])\n",
    "print(featureset_kupu[:3])\n",
    "\n",
    "# train the classifier\n",
    "nb_classifier = NaiveBayesClassifier.train(train_set_kupu)\n",
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our classifier on some kupu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kupu = 'maramatanga'\n",
    "\n",
    "#check that our test word isn't one that is already in the list of kupu that could be part of the training set\n",
    "print('Is \\'{}\\' in the training data?:'.format(test_kupu), test_kupu in [kupu for (kupu, reo) in labeled_text])\n",
    "\n",
    "# classify the kupu\n",
    "nb_classifier.classify(extract_features('maramatanga'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the accuracy of our classifier using the test set from the data set that we split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy:',classify.accuracy(nb_classifier, test_set_kupu),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can also print out the result of classifying some words from an untagged corpus with a mix of reo\n",
    "\n",
    "#load and format our untagged text\n",
    "with open('mixedWords.txt', 'r', encoding='utf8') as f:\n",
    "  mixedText = f.read().splitlines()\n",
    "  mixedText = [ast.literal_eval(item) for item in mixedText]\n",
    "\n",
    "print(mixedText[:5])\n",
    "print(len(mixedText))\n",
    "\n",
    "# apply the classifier and print some results\n",
    "for (word, tag) in mixedText[:20]:\n",
    "  print(\"Word:\",word, \" - Reo:\", nb_classifier.classify(extract_kupu_features(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do the results above compare with the accuracy as estimated by the classifier from the tagged data? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
